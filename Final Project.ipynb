{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc1f053",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f8a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d631826",
   "metadata": {},
   "source": [
    "## Resize images to be the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c3074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (64, 64)\n",
    "\n",
    "# Resize all images to be target_size.\n",
    "# First, resize so that the shortest side is the same as target_size,\n",
    "#   then crop with the center as the anchor.\n",
    "for root, dirs, files in os.walk(\"raw-img\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        with Image.open(filename) as im:\n",
    "            size = im.size\n",
    "            if size == target_size:\n",
    "                continue\n",
    "\n",
    "            longest = min(size)\n",
    "            scale = target_size[0]/longest\n",
    "            \n",
    "            if size[0] < size[1]:\n",
    "                resized_image = im.resize((target_size[0], int(size[1] * scale)), resample=Image.Resampling.BICUBIC)\n",
    "            else:\n",
    "                resized_image = im.resize((int(size[0] * scale), target_size[0]), resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "        x1 = (resized_image.size[0] - target_size[0]) // 2\n",
    "        y1 = (resized_image.size[1] - target_size[1]) // 2\n",
    "        x2 = x1 + target_size[0]\n",
    "        y2 = y1 + target_size[1]\n",
    "        cropped_image = resized_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        cropped_image.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd087f5",
   "metadata": {},
   "source": [
    "## Import and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ae73942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26179 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "class_names = (\"cane\", \"cavallo\", \"elefante\", \"farfalla\", \"gallina\", \"gatto\", \"mucca\", \"pecora\", \"ragno\", \"scoiattolo\")\n",
    "batch_size = 16\n",
    "train_val_split = 0.2\n",
    "\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"raw-img\",\n",
    "    image_size=target_size,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "size = len(ds)\n",
    "train_size, test_size = int(8/10 * size), int(2/10 * size)\n",
    "while train_size + test_size < size:\n",
    "    train_size += 1\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "val_ds = train_ds.take(int(0.1 * len(train_ds)))\n",
    "test_ds = ds.skip(train_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e701c3",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "865799b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_17 (Rescaling)    (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 31, 31, 32)        1568      \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 10, 10, 128)       73856     \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,770,666\n",
      "Trainable params: 1,770,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    # [64, 64, 3]\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "    # [31, 31, 32]\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "    # [29, 29, 64]\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "    # [14, 14, 64]\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "    # [12, 12, 64]\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "    # [10, 10, 64]\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 6400\n",
    "    #tf.keras.layers.Dense(units=6400, activation=\"relu\"),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Dense(units=1024, activation=\"relu\"),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "#cnn.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "cnn.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "cnn.build((None, target_size[0], target_size[1], 3))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33c73fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 2.1268 - accuracy: 0.2357 - val_loss: 1.9437 - val_accuracy: 0.3235\n",
      "Epoch 2/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.8483 - accuracy: 0.3591 - val_loss: 1.5964 - val_accuracy: 0.4628\n",
      "Epoch 3/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 1.6517 - accuracy: 0.4318 - val_loss: 1.5105 - val_accuracy: 0.4976\n",
      "Epoch 4/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.5368 - accuracy: 0.4733 - val_loss: 1.3174 - val_accuracy: 0.5463\n",
      "Epoch 5/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.4462 - accuracy: 0.5067 - val_loss: 1.2726 - val_accuracy: 0.5816\n",
      "Epoch 6/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.3650 - accuracy: 0.5316 - val_loss: 1.1380 - val_accuracy: 0.6059\n",
      "Epoch 7/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.2774 - accuracy: 0.5631 - val_loss: 1.0810 - val_accuracy: 0.6279\n",
      "Epoch 8/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.1962 - accuracy: 0.5876 - val_loss: 1.0204 - val_accuracy: 0.6479\n",
      "Epoch 9/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.1133 - accuracy: 0.6119 - val_loss: 0.9520 - val_accuracy: 0.6651\n",
      "Epoch 10/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 1.0394 - accuracy: 0.6382 - val_loss: 0.9122 - val_accuracy: 0.6732\n",
      "Epoch 11/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.9604 - accuracy: 0.6687 - val_loss: 0.7588 - val_accuracy: 0.7405\n",
      "Epoch 12/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.9057 - accuracy: 0.6815 - val_loss: 0.7210 - val_accuracy: 0.7615\n",
      "Epoch 13/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.8273 - accuracy: 0.7089 - val_loss: 0.6103 - val_accuracy: 0.7958\n",
      "Epoch 14/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.7566 - accuracy: 0.7281 - val_loss: 0.5438 - val_accuracy: 0.8177\n",
      "Epoch 15/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.7232 - accuracy: 0.7423 - val_loss: 0.4740 - val_accuracy: 0.8478\n",
      "Epoch 16/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.6741 - accuracy: 0.7619 - val_loss: 0.4587 - val_accuracy: 0.8502\n",
      "Epoch 17/30\n",
      "1310/1310 [==============================] - 11s 8ms/step - loss: 0.6192 - accuracy: 0.7773 - val_loss: 0.3924 - val_accuracy: 0.8678\n",
      "Epoch 18/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.5982 - accuracy: 0.7854 - val_loss: 0.3273 - val_accuracy: 0.8922\n",
      "Epoch 19/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.5542 - accuracy: 0.8005 - val_loss: 0.3241 - val_accuracy: 0.8974\n",
      "Epoch 20/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.5289 - accuracy: 0.8101 - val_loss: 0.4096 - val_accuracy: 0.8593\n",
      "Epoch 21/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.5087 - accuracy: 0.8160 - val_loss: 0.3046 - val_accuracy: 0.8960\n",
      "Epoch 22/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.5122 - accuracy: 0.8171 - val_loss: 0.2694 - val_accuracy: 0.9122\n",
      "Epoch 23/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4675 - accuracy: 0.8304 - val_loss: 0.2425 - val_accuracy: 0.9222\n",
      "Epoch 24/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4549 - accuracy: 0.8385 - val_loss: 0.2807 - val_accuracy: 0.9046\n",
      "Epoch 25/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4551 - accuracy: 0.8385 - val_loss: 0.2104 - val_accuracy: 0.9323\n",
      "Epoch 26/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4371 - accuracy: 0.8428 - val_loss: 0.1536 - val_accuracy: 0.9552\n",
      "Epoch 27/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4361 - accuracy: 0.8457 - val_loss: 0.1741 - val_accuracy: 0.9523\n",
      "Epoch 28/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.4112 - accuracy: 0.8537 - val_loss: 0.2236 - val_accuracy: 0.9327\n",
      "Epoch 29/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.3927 - accuracy: 0.8615 - val_loss: 0.1799 - val_accuracy: 0.9451\n",
      "Epoch 30/30\n",
      "1310/1310 [==============================] - 11s 9ms/step - loss: 0.3919 - accuracy: 0.8570 - val_loss: 0.1843 - val_accuracy: 0.9389\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e182f5",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "163dea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - 6s 6ms/step - loss: 2.1406 - accuracy: 0.6066\n"
     ]
    }
   ],
   "source": [
    "score = cnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e038fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
